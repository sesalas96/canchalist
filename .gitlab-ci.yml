stages:
  - snyk
  - sast
  - dependency-scanning
  - container-scanning
  - build
  - deploy-dev
  - deploy-qa
  - deploy-stage
  - check-jira
  - deploy-prod

variables:
  EKS_CLUSTER_NAME: cci-web-dev
  EKS_REGION: us-east-1
  DOCKER_IMAGE_REPO: 067322660699.dkr.ecr.us-east-1.amazonaws.com/cci-node-template
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: ""
  JIRA_BASE_URL: "https://jirasoft.consolidated.com/"
  JIRA_PROJECT_KEY: "DTPR"
  JIRA_PAT: "NjQ0NjU5NjYyMDAwOtp1ETuto83ov/idBIBUsgadPbkM"
  SNYK_TOKEN: "SNYK_TOKEN"
  FEATURE_FLAG_SNYK: "false"
  FEATURE_FLAG_SAST: "false"
  FEATURE_FLAG_DEPENDENCY_SCANNING: "false"
  FEATURE_FLAG_CONTAINER_SCANNING: "false"
  FEATURE_FLAG_CHECK_JIRA: "false"

snyk:
  stage: snyk
  image: snyk/snyk-cli:docker
  script: |
    if [[ "$FEATURE_FLAG_SNYK" == "true" ]]; then
      snyk auth $SNYK_TOKEN
      snyk test --docker $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA | tee snyk_sast.log
    else
      echo "Skipping Snyk scan as FEATURE_FLAG_SNYK is set to false."
    fi
  artifacts:
    paths:
      - snyk_sast.log
  only:
    - main
  retry: 2  # Retry the job up to 2 times on failure

sast:
  stage: sast
  image: snyk/snyk-cli:docker
  script: |
    if [[ "$FEATURE_FLAG_SAST" == "true" ]]; then
      ./run_sast.sh | tee sast.log
    else
      echo "Skipping SAST scan as FEATURE_FLAG_SAST is set to false."
    fi
  artifacts:
    paths:
      - sast.log
  only:
    - main
  retry: 2  # Retry the job up to 2 times on failure

dependency-scanning:
  stage: dependency-scanning
  image: snyk/snyk-cli:docker
  script: |
    if [[ "$FEATURE_FLAG_DEPENDENCY_SCANNING" == "true" ]]; then
      snyk auth $SNYK_TOKEN
      snyk test --file=package.json | tee dependency_scanning.log
    else
      echo "Skipping Dependency Scanning as FEATURE_FLAG_DEPENDENCY_SCANNING is set to false."
    fi
  artifacts:
    paths:
      - dependency_scanning.log
  only:
    - main
  retry: 2  # Retry the job up to 2 times on failure

container-scanning:
  stage: container-scanning
  image: snyk/snyk-cli:docker
  script: |
    if [[ "$FEATURE_FLAG_CONTAINER_SCANNING" == "true" ]]; then
      snyk auth $SNYK_TOKEN
      snyk container test $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA | tee container_scanning.log
    else
      echo "Skipping Container Scanning as FEATURE_FLAG_CONTAINER_SCANNING is set to false."
    fi
  artifacts:
    paths:
      - container_scanning.log
  only:
    - main
  retry: 2

build:
  stage: build
  image: docker:20.10.16
  tags:
    - techtransform
  services:
    - name: docker:20.10.16-dind
      alias: docker
      command: ["--privileged", "--insecure-registry=git.ipnms.net:5050"]
  before_script:
    - apk update && apk add --no-cache curl jq python3 py3-pip unzip
    - pip3 install awscli --upgrade
    - export PATH=$HOME/.local/bin:$PATH
    - curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
    - mkdir -p ~/.kube
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    - export AWS_DEFAULT_REGION=$EKS_REGION
    - echo "Checking AWS credentials and region "
    - aws sts get-caller-identity || (echo "AWS credentials are invalid" && exit 1)
    - echo "$AWS_DEFAULT_REGION"
    - echo "$EKS_CLUSTER_NAME"
    - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $EKS_CLUSTER_NAME || (echo "Failed to update kubeconfig" && exit 1)
    - echo "Kubeconfig file content:"
    - cat ~/.kube/config # Debugging step to check the kubeconfig file
    - echo "Available contexts:"
    - kubectl config get-contexts # Debugging step to check available contexts
  script:
    - "echo \"Tag: $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA\""
    - docker build -t $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA .
    - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $DOCKER_IMAGE_REPO
    - docker push $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA
  artifacts:
    paths:
      - build.log
  only:
    - main

deploy-dev:
  stage: deploy-dev
  image: bitnami/kubectl:latest
  environment:
    name: dev
  before_script:
    - mkdir -p ~/.kube
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    - export AWS_DEFAULT_REGION=$EKS_REGION
    - export IMAGE_PATH=$DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA
    - aws eks --region $EKS_REGION update-kubeconfig --name $EKS_CLUSTER_NAME || (echo "Failed to update kubeconfig" && exit 1)
    - cat ~/.kube/config
    - kubectl config view
  script:
    - KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME | head -n 1)
    - kubectl config use-context "$KUBE_CONTEXT" || (echo "Failed to switch context" && exit 1)
    - kubectl config set-context --current --namespace=dev
    - envsubst < k8s/deployment.yaml | kubectl apply -f -
    - envsubst < k8s/service.yaml | kubectl apply -f -
    - envsubst < k8s/ingress.yaml | kubectl apply -f -
  only:
    - main
  when: manual
  allow_failure: false

deploy-qa:
  stage: deploy-qa
  image: bitnami/kubectl:latest
  environment:
    name: qa
  script: |
    export KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME)
    echo "$KUBE_CONTEXT"
    kubectl config use-context $KUBE_CONTEXT || (echo "Failed to switch context" && exit 1)
    kubectl config set-context --current --namespace=qa
    kubectl apply -f k8s/deployment.yaml
    kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false

deploy-stage:
  stage: deploy-stage
  image: bitnami/kubectl:latest
  environment:
    name: stage
  script: |
    export KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME)
    echo "$KUBE_CONTEXT"
    kubectl config use-context $KUBE_CONTEXT || (echo "Failed to switch context" && exit 1)
    kubectl config set-context --current --namespace=stage
    kubectl apply -f k8s/deployment.yaml
    kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false

check-jira:
  stage: check-jira
  image: curlimages/curl:latest
  script: |
    if [[ "$FEATURE_FLAG_CHECK_JIRA" == "true" ]]; then
      JIRA_ISSUE_STATUS=$(curl -s -H "Authorization: Bearer $JIRA_PAT" -X GET -H "Content-Type: application/json" "$JIRA_BASE_URL/rest/api/2/search?jql=project=$JIRA_PROJECT_KEY%20AND%20status%20in%20(Testing)")
      echo "$JIRA_ISSUE_STATUS"
      if [[ "$JIRA_ISSUE_STATUS" == *"Testing"* ]]; then
        echo "Jira issues are ready for deployment."
      else
        echo "No Jira issues are ready for deployment." >&2
        exit 1
      fi
    else
      echo "Skipping Jira check as FEATURE_FLAG_CHECK_JIRA is set to false."
    fi
  only:
    - main
  when: manual
  allow_failure: false

deploy-prod:
  stage: deploy-prod
  image: bitnami/kubectl:latest
  environment:
    name: production
  script: |
    export KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME)
    echo "$KUBE_CONTEXT"
    kubectl config use-context $KUBE_CONTEXT || (echo "Failed to switch context" && exit 1)
    kubectl config set-context --current --namespace=production
    kubectl apply -f k8s/deployment.yaml
    kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false
