stages:
  - snyk
  - sast
  - dependency-scanning
  - container-scanning
  - iac-security
  - build
  - deploy-dev
  - deploy-qa
  - deploy-stage
  - check-jira
  - deploy-prod

variables:
  EKS_CLUSTER_NAME: cci-web-dev
  EKS_REGION: us-east-1
  DOCKER_IMAGE_REPO: 067322660699.dkr.ecr.us-east-1.amazonaws.com/cci-node-template
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: ""
  JIRA_BASE_URL: "https://jirasoft.consolidated.com/"
  JIRA_PROJECT_KEY: "DTPR"
  JIRA_PAT: "NjQ0NjU5NjYyMDAwOtp1ETuto83ov/idBIBUsgadPbkM"
  SNYK_TOKEN: "SNYK_TOKEN"

# snyk:
#   stage: snyk
#   image: snyk/snyk-cli:docker
#   script:
#     - snyk auth $SNYK_TOKEN
#     - snyk test --docker $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA | tee snyk_sast.log
#   artifacts:
#     paths:
#       - snyk_sast.log
#   only:
#     - main

# sast:
#   stage: sast
#   image: snyk/snyk-cli:docker
#   script:
#     - ./run_sast.sh | tee sast.log
#   artifacts:
#     paths:
#       - sast.log
#   only:
#     - main

# dependency-scanning:
#   stage: dependency-scanning
#   image: snyk/snyk-cli:docker
#   script:
#     - snyk auth $SNYK_TOKEN
#     - snyk test --file=package.json | tee dependency_scanning.log
#   artifacts:
#     paths:
#       - dependency_scanning.log
#   only:
#     - main

# container-scanning:
#   stage: container-scanning
#   image: snyk/snyk-cli:docker
#   script:
#     - snyk auth $SNYK_TOKEN
#     - snyk container test $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA | tee container_scanning.log
#   artifacts:
#     paths:
#       - container_scanning.log
#   only:
#     - main

iac-security:
  stage: iac-security
  image: python:3.9-slim
  before_script:
    - apt-get update && apt-get install -y python3-pip
    - pip3 install --user checkov
    - export PATH=$HOME/.local/bin:$PATH
  script:
    - checkov -d . | tee iac_security.log
  artifacts:
    paths:
      - iac_security.log
  only:
    - main

build:
  stage: build
  image: docker:20.10.16
  tags:
    - techtransform
  services:
    - name: docker:20.10.16-dind
      alias: docker
      command: ["--privileged", "--insecure-registry=git.ipnms.net:5050"]
  before_script:
    - apk update && apk add --no-cache curl jq python3 py3-pip unzip
    - pip3 install awscli --upgrade
    - export PATH=$HOME/.local/bin:$PATH
    - curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
    - mkdir -p ~/.kube
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    - export AWS_DEFAULT_REGION=$EKS_REGION
    - echo "Checking AWS credentials and region "
    - aws sts get-caller-identity || (echo "AWS credentials are invalid" && exit 1)
    - echo "$AWS_DEFAULT_REGION"
    - echo "$EKS_CLUSTER_NAME"
    - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $EKS_CLUSTER_NAME || (echo "Failed to update kubeconfig" && exit 1)
    - echo "Kubeconfig file content:"
    - cat ~/.kube/config # Debugging step to check the kubeconfig file
    - echo "Available contexts:"
    - kubectl config get-contexts # Debugging step to check available contexts
  script:
    - "echo \"Tag: $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA\""
    - docker build -t $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA .
    - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin $DOCKER_IMAGE_REPO
    - docker push $DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA
  artifacts:
    paths:
      - build.log
  only:
    - main
  needs: ["iac-security"] # Need SNYK Token before this works add "snyk", "sast", "dependency-scanning", "container-scanning" when available

deploy-dev:
  stage: deploy-dev
  image: bitnami/kubectl:latest
  environment:
    name: dev
  before_script:
    - mkdir -p ~/.kube
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    - export AWS_DEFAULT_REGION=$EKS_REGION
    - export IMAGE_PATH=$DOCKER_IMAGE_REPO:$CI_COMMIT_SHORT_SHA
    - aws eks --region $EKS_REGION update-kubeconfig --name $EKS_CLUSTER_NAME || (echo "Failed to update kubeconfig" && exit 1)
    - cat ~/.kube/config
    - kubectl config view
  script:
    - KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME | head -n 1)
    - kubectl config use-context "$KUBE_CONTEXT" || (echo "Failed to switch context" && exit 1)
    - kubectl config set-context --current --namespace=dev
    - envsubst < k8s/deployment.yaml | kubectl apply -f -
    - envsubst < k8s/service.yaml | kubectl apply -f -
    - envsubst < k8s/ingress.yaml | kubectl apply -f -
  only:
    - main
  when: manual
  allow_failure: false
  # needs: ["snyk"]

deploy-qa:
  stage: deploy-qa
  image: bitnami/kubectl:latest
  environment:
    name: qa
  script:
    - export KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME)
    - echo "$KUBE_CONTEXT"
    - kubectl config use-context $KUBE_CONTEXT || (echo "Failed to switch context" && exit 1)
    - kubectl config set-context --current --namespace=qa
    - kubectl apply -f k8s/deployment.yaml
    - kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false
  # needs: ["snyk"]

deploy-stage:
  stage: deploy-stage
  image: bitnami/kubectl:latest
  environment:
    name: stage
  script:
    - export KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME)
    - echo "$KUBE_CONTEXT"
    - kubectl config use-context $KUBE_CONTEXT || (echo "Failed to switch context" && exit 1)
    - kubectl config set-context --current --namespace=stage
    - kubectl apply -f k8s/deployment.yaml
    - kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false
  # needs: ["snyk"]

# check-jira:
#   stage: check-jira
#   image: curlimages/curl:latest
#   script:
#     - >
#       JIRA_ISSUE_STATUS=$(curl -s -H "Authorization: Bearer $JIRA_PAT" -X GET
#       -H "Content-Type: application/json"
#       "$JIRA_BASE_URL/rest/api/2/search?jql=project=$JIRA_PROJECT_KEY%20AND%20status%20in%20(Testing)")
#     - echo "$JIRA_ISSUE_STATUS"
#     - >
#       if [[ "$JIRA_ISSUE_STATUS" == *"Testing"* ]]; then
#         echo "Jira issues are ready for deployment."
#       else
#         echo "No Jira issues are ready for deployment." >&2
#         exit 1
#       fi
#   only:
#     - main
#   when: manual
#   allow_failure: false
#   needs: ["snyk"]

deploy-prod:
  stage: deploy-prod
  image: bitnami/kubectl:latest
  environment:
    name: production
  script:
    - export KUBE_CONTEXT=$(kubectl config get-contexts -o name | grep $EKS_CLUSTER_NAME)
    - echo "$KUBE_CONTEXT"
    - kubectl config use-context $KUBE_CONTEXT || (echo "Failed to switch context" && exit 1)
    - kubectl config set-context --current --namespace=production
    - kubectl apply -f k8s/deployment.yaml
    - kubectl apply -f k8s/service.yaml
  # dependencies:
  #   - check-jira
  only:
    - main
  when: manual
  allow_failure: false
  # needs: ["snyk", "check-jira"]
