stages:
  - build
  - deploy-dev
  - deploy-qa
  - deploy-uat
  - deploy-prod

variables:
  EKS_CLUSTER_NAME: cci-web-dev
  EKS_REGION: us-east-1
  DOCKER_IMAGE: 067322660699.dkr.ecr.us-east-1.amazonaws.com/cci-node-template
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: ""

before_script:
  - apt-get update && apt-get install -y curl jq python3 python3-pip unzip
  - curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  - unzip awscliv2.zip
  - ./aws/install --bin-dir $HOME/bin --install-dir $HOME/aws-cli --update
  - export PATH=$HOME/bin:$PATH
  - mkdir -p $HOME/bin
  - curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
  - chmod +x ./kubectl
  - mv ./kubectl $HOME/bin/kubectl
  - export PATH=$HOME/bin:$PATH
  - mkdir -p ~/.kube
  - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
  - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
  - export AWS_DEFAULT_REGION=$EKS_REGION
  - export AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN
  - echo "Checking AWS credentials and region"
  - aws sts get-caller-identity || (echo "AWS credentials are invalid" && exit 1)
  - echo "$AWS_DEFAULT_REGION"
  - export EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME
  - echo "$EKS_CLUSTER_NAME"
  - kubectl config use-context $EKS_CLUSTER_NAME
  - kubectl create namespace production || true
  - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $EKS_CLUSTER_NAME || (echo "Failed to update kubeconfig" && exit 1)
  - kubectl cluster-info || (echo "Failed to connect to cluster" && exit 1)

build:
  stage: build
  image: docker:20.10.16
  services:
    - name: docker:20.10.16-dind
      command: ["--privileged"]
  variables:
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  script:
    - docker info
    - docker build --cache-from $DOCKER_IMAGE -t $DOCKER_IMAGE .
    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
    - docker push $DOCKER_IMAGE
  only:
    - main

deploy-dev:
  stage: deploy-dev
  image: bitnami/kubectl:latest
  environment:
    name: dev
  script:
    - |
      kubectl config use-context $EKS_CLUSTER_NAME
      kubectl config set-context --current --namespace=dev
      kubectl apply -f k8s/deployment.yaml
      kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false

deploy-qa:
  stage: deploy-qa
  image: bitnami/kubectl:latest
  environment:
    name: qa
  script:
    - |
      kubectl config use-context $EKS_CLUSTER_NAME
      kubectl config set-context --current --namespace=qa
      kubectl apply -f k8s/deployment.yaml
      kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false

deploy-uat:
  stage: deploy-uat
  image: bitnami/kubectl:latest
  environment:
    name: uat
  script:
    - |
      kubectl config use-context $EKS_CLUSTER_NAME
      kubectl config set-context --current --namespace=uat
      kubectl apply -f k8s/deployment.yaml
      kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false

deploy-prod:
  stage: deploy-prod
  image: bitnami/kubectl:latest
  environment:
    name: production
  script:
    - |
      kubectl config use-context $EKS_CLUSTER_NAME
      kubectl config set-context --current --namespace=production
      kubectl apply -f k8s/deployment.yaml
      kubectl apply -f k8s/service.yaml
  only:
    - main
  when: manual
  allow_failure: false
